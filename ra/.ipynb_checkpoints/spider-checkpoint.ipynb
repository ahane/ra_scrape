{
 "metadata": {
  "name": "",
  "signature": "sha256:24e5c52f034c13a5757d7d5b9d53a9143915fdaed146f426dc909df9ce82fc22"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile ra/items.py\n",
      "import scrapy\n",
      "\n",
      "class Event(scrapy.Item):\n",
      "    #item_type = scrapy.Field(default='event')\n",
      "    item_type = scrapy.Field()\n",
      "    ra_event_id = scrapy.Field()\n",
      "    ra_url = scrapy.Field()\n",
      "    name = scrapy.Field()    \n",
      "    start_datetime = scrapy.Field()\n",
      "    end_datetime = scrapy.Field()\n",
      "    #ra_club_id = scrapy.Field()\n",
      "    \n",
      "    #these fields are for a document-like serving\n",
      "    club = scrapy.Field()\n",
      "    artists = scrapy.Field()\n",
      "    \n",
      "class Club(scrapy.Item):\n",
      "    item_type = scrapy.Field(default='club')\n",
      "    ra_club_id = scrapy.Field()\n",
      "    ra_url = scrapy.Field()\n",
      "    name = scrapy.Field()\n",
      "    adress = scrapy.Field()\n",
      "    lat = scrapy.Field()\n",
      "    lon = scrapy.Field()\n",
      "    ra_locale_id = scrapy.Field()\n",
      "\n",
      "class Performance(scrapy.Item):\n",
      "    item_type = scrapy.Field(default='performance')\n",
      "    ra_event_id = scrapy.Field()\n",
      "    sc_artist_id = scrapy.Field()\n",
      "    artist = scrapy.Field()\n",
      "    \n",
      "class Artist(scrapy.Item):\n",
      "    item_type = scrapy.Field(default='artist')\n",
      "    ra_artist_id = scrapy.Field()\n",
      "    ra_url = scrapy.Field()\n",
      "    name = scrapy.Field()\n",
      "    sc_user = scrapy.Field()\n",
      "    sc_url = scrapy.Field()\n",
      "    #sc_track_permalink = scrapy.Field()\n",
      "    #sc_track_id = scrapy.Field()\n",
      "    #sc_value = scrapy.Field() #arbitray number for rankin artists\n",
      "    \n",
      "class Track(scrapy.Item):\n",
      "    artist_id = scrapy.Field()\n",
      "    sc_track_id = scrapy.Field()\n",
      "    sc_user_name = scrapy.Field()\n",
      "    sc_num_plays = scrapy.Field()\n",
      "    sc_description = scrapy.Field()\n",
      "    sc_genre = scrapy.Field()\n",
      "    sc_embeddable_by = scrapy.Field()\n",
      "    sc_lable_name = scrapy.Field()\n",
      "    sc_last_modiefied = scrapy.Field()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting ra/items.py\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 250
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile ra/spiders/event_spider.py\n",
      "import scrapy\n",
      "from scrapy.contrib.spiders import CrawlSpider, Rule\n",
      "from scrapy.contrib.linkextractors import LinkExtractor\n",
      "scrapy.contrib.linkextractors.lxmlhtml.LxmlLinkExtractor\n",
      "from ra.items import Event, Club, Performance, Artist\n",
      "import urllib\n",
      "import datetime\n",
      "import re\n",
      "import json\n",
      "import geocoder\n",
      "from collections import defaultdict\n",
      "BASE_URL = 'http://www.residentadvisor.net'\n",
      "LISTINGS_EXT = '/events.aspx?'\n",
      "BERLIN_AI = 34\n",
      "TODAY = datetime.date.today()\n",
      "dates = [TODAY, TODAY + datetime.timedelta(1), TODAY + datetime.timedelta(2)]\n",
      "listings_params = [{'ai': BERLIN_AI,\n",
      "                  'v': 'day',\n",
      "                  'mn': d.month,\n",
      "                  'yr': d.year,\n",
      "                  'dy': d.day} for d in dates]\n",
      "\n",
      "\n",
      "listings_urls = [BASE_URL + LISTINGS_EXT + urllib.urlencode(p) for p in listings_params]\n",
      "\n",
      "\n",
      "# Helper Functions\n",
      "def datetimes_from_date_div(date_div):\n",
      "    ''' Extracts start and end datetimes from\n",
      "        the resident advisor date div\n",
      "    '''\n",
      "    \n",
      "    links = date_div.xpath('a/@href').extract()\n",
      "    num_date_links = len(links)\n",
      "    start_date = date_from_events_url(links[0])\n",
      "    end_date = None\n",
      "    if num_date_links > 1:\n",
      "        end_date = date_from_events_url(links[-1])\n",
      "    \n",
      "    start_time, end_time = times_from_str(date_div.extract())\n",
      "    \n",
      "    start_datetime, end_datetime = join_times_dates(start_date, end_date, start_time, end_time)\n",
      "    \n",
      "    return start_datetime, end_datetime\n",
      "\n",
      "def date_from_events_url(url):\n",
      "    from urlparse import parse_qs, urlparse\n",
      "    from datetime import date\n",
      "    date_dict = parse_qs(urlparse(url).query, keep_blank_values=True)\n",
      "    try:\n",
      "        y = int(date_dict['yr'][0])\n",
      "        m = int(date_dict['mn'][0])\n",
      "        d = int(date_dict['dy'][0])\n",
      "    except:\n",
      "        raise\n",
      "    return date(y, m , d)\n",
      "\n",
      "def times_from_str(string):\n",
      "    '''\n",
      "        Looks for '12:00 - 13:00' in a string and returns\n",
      "        both times as python time objects.\n",
      "    '''\n",
      "    def time_from_str(time_str):\n",
      "        return datetime.datetime.strptime(time_str, '%H:%M').time()\n",
      "    \n",
      "    def search_start_end(string):\n",
      "        match = re.search('(\\d\\d\\:\\d\\d)\\s\\-\\s(\\d\\d\\:\\d\\d)', string)\n",
      "        if match:\n",
      "            start_str = match.group(1)\n",
      "            start_time = time_from_str(start_str)\n",
      "            \n",
      "            end_str = match.group(2)\n",
      "            end_time = time_from_str(end_str)\n",
      "        else:\n",
      "            raise KeyError('Coulnt find start and end times')\n",
      "        \n",
      "        return start_time, end_time\n",
      "            \n",
      "    def search_start(string):\n",
      "        match = re.search('(\\d\\d\\:\\d\\d)', string)\n",
      "        if match:\n",
      "            start_str = match.group(1)\n",
      "            start_time = time_from_str(start_str)\n",
      "       \n",
      "        else:\n",
      "            raise KeyError('Couldnt find start time')\n",
      "        \n",
      "        return start_time\n",
      "    \n",
      "    start_time, end_time = None, None\n",
      "    \n",
      "    try:\n",
      "        start_time, end_time = search_start_end(string)\n",
      "    except KeyError:\n",
      "        try:\n",
      "            start_time = search_start(string)\n",
      "        except KeyError:\n",
      "            pass\n",
      "    return start_time, end_time\n",
      "\n",
      "def join_times_dates(start_date, end_date, start_time, end_time):\n",
      "    ''' Takes one or more dates and two times and turns them\n",
      "        into a start datetime and an end datetime.\n",
      "    '''    \n",
      "    \n",
      "    DEFAULT_START_TIME = '23:59'\n",
      "    DEFAULT_DURATION = 6\n",
      "    \n",
      "    from datetime import datetime, timedelta\n",
      "    \n",
      "    \n",
      "    def same_day(st, et):\n",
      "        if not st or not et:\n",
      "            return True\n",
      "        else:\n",
      "            return st <= et\n",
      "    \n",
      "    def datetime_from_date_time(date, time):\n",
      "        dt = datetime(date.year, date.month, date.day, time.hour, time.minute)\n",
      "        return dt\n",
      "    \n",
      "    if not start_time:\n",
      "        start_time  = datetime.strptime(DEFAULT_START_TIME, '%H:%M').time()\n",
      "    \n",
      "    start_datetime = datetime_from_date_time(start_date, start_time)\n",
      "    \n",
      "    \n",
      "    # If we didn't receive a end_date and the end time is\n",
      "    # after the start time, the event ends on the same day.\n",
      "    if not end_date and same_day(start_time, end_time):\n",
      "        end_date = start_date\n",
      "    \n",
      "    # If no end_date and end time is before start time\n",
      "    # the event ends one day after the start date\n",
      "    elif not end_date and not same_day(start_time, end_time):\n",
      "        end_date = start_date + timedelta(1) #add one day\n",
      "    \n",
      "    # Otherwise we use the end_date we got passed\n",
      "    else:\n",
      "        pass\n",
      "    \n",
      "    if not end_time:\n",
      "        duration = timedelta(hours=DEFAULT_DURATION)\n",
      "        end_datetime = start_datetime + duration\n",
      "        end_time = end_datetime.time()\n",
      "    \n",
      "    end_datetime = datetime_from_date_time(end_date, end_time)\n",
      "    \n",
      "    return start_datetime, end_datetime\n",
      "\n",
      "# Actual Spider:\n",
      "class RAEventSpider(CrawlSpider):\n",
      "    name = 'event_spider'\n",
      "    allowed_domains = [BASE_URL, 'www.residentadvisor.net', 'api.soundcloud.com']\n",
      "    start_urls = listings_urls\n",
      "    locale = listings_params[0]['ai']\n",
      "    rules = [\n",
      "        Rule(LinkExtractor(allow=(r'\\/event\\.aspx\\?',), canonicalize=False),\n",
      "             callback='parse_event'),\n",
      "    ]\n",
      "\n",
      "    extract_digits = re.compile(r'(\\d+)')\n",
      "     \n",
      "    def parse_event(self, response):\n",
      "        '''\n",
      "            @url http://www.residentadvisor.net/events.aspx?ai=34&v=day&mn=10&yr=2014&dy=20\n",
      "            @returns requests 5\n",
      "            @returns items 1\n",
      "        '''\n",
      "        \n",
      "        event = Event()\n",
      "        event['ra_url'] = response.url    \n",
      "        event['ra_event_id'] = self.extract_digits.search(event['ra_url']).group(1)\n",
      "        event['item_type'] = 'event'\n",
      "        \n",
      "        event_title = response.xpath(\"//div[@id = 'sectionHead']/h1/text()\").extract()[0]\n",
      "        event['name'] = re.match('(.+)\\sat.+$', event_title).group(1) #remove 'at LOCATION'\n",
      "        \n",
      "        date_div = response.xpath(\"//div[text()='Date /']/..\")[0]\n",
      "        \n",
      "        start, end = datetimes_from_date_div(date_div)\n",
      "        event['start_datetime'], event['end_datetime'] = start.isoformat(), end.isoformat()\n",
      "        event['artists'] = []\n",
      "       \n",
      "        club_link = response.xpath(\"//a[contains(@title, 'Club profile')]\")\n",
      "        if club_link:\n",
      "            club = Club()\n",
      "            club['ra_locale_id'] = self.locale\n",
      "            club['ra_url'] = BASE_URL + '/' + club_link.xpath(\"@href\").extract()[0]\n",
      "            club['name'] = club_link.xpath(\"text()\").extract()[0]\n",
      "            id_match = self.extract_digits.search(club['ra_url'])\n",
      "            #self.log(type(id_match))\n",
      "            club['ra_club_id'] = id_match.group(1)\n",
      "            club['adress'] = club_link.xpath(\"../text()\").extract()[0]\n",
      "            \n",
      "            ## Todo: move geocoding into sperate scraper\n",
      "            geocode = geocoder.google(club['adress'])\n",
      "            if geocode.status_description == 'OK':\n",
      "\n",
      "                club['lat'] = geocode.latlng[0]\n",
      "                club['lon'] = geocode.latlng[1]\n",
      "            #event['ra_club_id'] = club['ra_club_id']\n",
      "            event['club'] = club\n",
      "        \n",
      "        lineup_selector = response.css(\".lineup\").xpath(\"a\")\n",
      "        if lineup_selector:\n",
      "            num_artists = len(lineup_selector)\n",
      "            for link_sel in lineup_selector:\n",
      "                artist = Artist()\n",
      "                url_ext = link_sel.xpath(\"@href\").extract()[0]\n",
      "                if url_ext[:4] == \"/dj/\":\n",
      "                    artist['ra_url'] = BASE_URL + url_ext\n",
      "                    artist['name'] = link_sel.xpath(\"text()\").extract()[0]\n",
      "                    artist['ra_artist_id'] = url_ext.split(\"/\")[2]\n",
      "                    \n",
      "                    #step into RA artist page\n",
      "                    request = scrapy.Request(artist['ra_url'], callback=self.parse_artist_page)\n",
      "                    request.meta['event'] = event\n",
      "                    request.meta['artist'] = artist\n",
      "                    request.meta['num_artists'] = num_artists\n",
      "                    yield request\n",
      "        else:\n",
      "            yield event\n",
      "    \n",
      "    def parse_artist_page(self, response):\n",
      "        \n",
      "        artist = response.meta['artist']\n",
      "        event = response.meta['event']\n",
      "        \n",
      "        sc_link_sel = response.xpath(\"//a[contains(@href, 'http://www.soundcloud.com')][contains(text(), 'SoundCloud')]/@href\")\n",
      "        if sc_link_sel:\n",
      "            artist['sc_url'] = sc_link_sel.extract()[0]\n",
      "            artist['sc_user'] = artist['sc_url'].split('/')[-1]\n",
      "        \n",
      "        event['artists'] = event['artists'] + [artist]\n",
      "        \n",
      "        if response.meta['num_artists'] == len(event['artists']):\n",
      "           # from scrapy.shell import inspect_response\n",
      "           # inspect_response(response)\n",
      "            yield event    \n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting ra/spiders/event_spider.py\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile ra/spiders/sc_tracks.py\n",
      "import urllib\n",
      "import json\n",
      "from ra.items import Track\n",
      "import scrapy\n",
      "from scrapy.contrib.spiders import CrawlSpider, Rule\n",
      "# artist_id = scrapy.Field()\n",
      "# sc_track_id = scrapy.Field()\n",
      "# sc_user_name = scrapy.Field()\n",
      "# sc_num_plays = scrapy.Field()\n",
      "# sc_description = scrapy.Field()\n",
      "# sc_genre = scrapy.Field()\n",
      "# sc_embeddable_by = scrapy.Field()\n",
      "# sc_lable_name = scrapy.Field()\n",
      "# sc_last_modiefied = scrapy.Field()\n",
      "class SoundCloudSpider(CrawlSpider):\n",
      "    name = 'sc_track_spider'\n",
      "    allowed_domains = ['api.soundcloud.com']\n",
      "\n",
      "    def __init__(self):\n",
      "        with open('sc_cl_id.txt', 'r') as f:\n",
      "            SC_CLIENT_ID = f.read().rstrip()\n",
      "            sc_client_id = urllib.urlencode({\"client_id\": SC_CLIENT_ID})\n",
      "        TRACKS_URL = \"http://api.soundcloud.com/users/%s/tracks.json?\"+sc_client_id\n",
      "        \n",
      "        self.users_dict = get_sc_users_without_tracks()\n",
      "        self.user_names = users_dict.keys()\n",
      "        \n",
      "        self.start_urls = [(TRACKS_URL % user) for user in user_names]\n",
      "\n",
      "    def parse(self, response):\n",
      "        tracks = json.loads(response.body)\n",
      "        if tracks:\n",
      "            for track_dict in tracks:\n",
      "                track_dict =  defaultdict(int, track_dict) # some track miss fields\n",
      "            track = Track()\n",
      "            track['sc_user_name'] = track_dict['user']['username']\n",
      "            track['artist_id'] = users_dict[track['sc_user_name']] #we look up the internal artist id\n",
      "            track['sc_track_id'] = track_dict['id']\n",
      "            track['sc_num_plays'] = track_dict['playback_count']\n",
      "            track['sc_description'] = track_dict['description']\n",
      "            track['sc_genre'] = track_dict['genre']\n",
      "            track['sc_embeddable_by'] = track_dict['embeddable_by']\n",
      "            track['sc_lable_name'] = track_dict['label_name']\n",
      "            track['sc_last_modiefied'] = track_dict['last_modiefied']\n",
      "            yield track\n",
      "            \n",
      "    def get_sc_users_without_tracks(self):\n",
      "        from db.dbmodel import db_connect, create_tables, ArtistPage, ThirdParty, ArtistSample\n",
      "        from sqlalchemy.orm import sessionmaker\n",
      "        \n",
      "        engine = db_connect()\n",
      "        create_tables(engine)\n",
      "        Session = sessionmaker(bind=engine)\n",
      "        s = Session()\n",
      "\n",
      "        left_outer = s.query(ArtistPage).join(ThirdParty).outerjoin(\n",
      "                        ArtistSample, ArtistPage.artist_id == ArtistSample.artist_id\n",
      "                        ).filter(ArtistSample.sample_id == None, ThirdParty.name == 'SoundCloud')\n",
      "\n",
      "        users = {a.page_id: a.artist_id for a in left_outer}\n",
      "\n",
      "        s.close()\n",
      "        return users\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing ra/spiders/sc_tracks.py\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "a = requests.get('http://www.residentadvisor.net/event.aspx?631670')\n",
      "response = HtmlResponse(a.url, body=a.content, encoding='utf8')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'HtmlResponse' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-0f00e9fe74fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://www.residentadvisor.net/event.aspx?631670'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHtmlResponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'HtmlResponse' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = 'http://www.residentadvisor.net'\n",
      "extract_digits = re.compile(r'(\\d+)')\n",
      "event = Event()\n",
      "event['ra_url'] = response.url    \n",
      "event['ra_event_id'] = extract_digits.search(event['ra_url']).group(1)\n",
      "event['item_type'] = 'event'\n",
      "event['name'] = response.xpath(\"//div[@id = 'sectionHead']/h1/text()\").extract()[0]\n",
      "event['date'] = None #TODAY.isoformat()\n",
      "event['artists'] = []\n",
      "\n",
      "club_link = response.xpath(\"//a[contains(@title, 'Club profile')]\")\n",
      "if club_link:\n",
      "    club = Club()\n",
      "    club['ra_url'] = BASE_URL + '/' + club_link.xpath(\"@href\").extract()[0]\n",
      "    club['name'] = club_link.xpath(\"text()\").extract()[0]\n",
      "    id_match = extract_digits.search(club['ra_url'])\n",
      "    #self.log(type(id_match))\n",
      "    club['ra_club_id'] = id_match.group(1)\n",
      "    club['adress'] = club_link.xpath(\"../text()\").extract()[0]\n",
      "\n",
      "    #geocode = geocoder.google(club['adress'])\n",
      "    #if geocode.status_description == 'OK':\n",
      "    club['latlon'] = [0, 0] #geocode.latlng\n",
      "    #event['ra_club_id'] = club['ra_club_id']\n",
      "    event['club'] = club\n",
      "\n",
      "lineup_selector = response.css(\".lineup\").xpath(\"a\")\n",
      "if lineup_selector:\n",
      "    num_artists = len(lineup_selector)\n",
      "    for link_sel in lineup_selector:\n",
      "        artist = Artist()\n",
      "        url_ext = link_sel.xpath(\"@href\").extract()[0]\n",
      "        if url_ext[:4] == \"/dj/\":\n",
      "            artist['ra_url'] = BASE_URL + url_ext\n",
      "            artist['name'] = link_sel.xpath(\"text()\").extract()[0]\n",
      "            artist['ra_artist_id'] = url_ext.split(\"/\")[2]\n",
      "\n",
      "            #step into RA artist page\n",
      "            #request = scrapy.Request(artist['ra_url'], callback=self.parse_artist_page)\n",
      "            #request.meta['event'] = event\n",
      "            #request.meta['artist'] = artist\n",
      "            #request.meta['num_artists'] = num_artists"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def date_from_events_url(url):\n",
      "    from urlparse import parse_qs, urlparse\n",
      "    from datetime import date\n",
      "    date_dict =parse_qs(urlparse(url).query, keep_blank_values=True)\n",
      "    try:\n",
      "        y = int(date_dict['yr'][0])\n",
      "        m = int(date_dict['mn'][0])\n",
      "        d = int(date_dict['dy'][0])\n",
      "    except:\n",
      "        raise\n",
      "    return date(y, m , d)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 234
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 210
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sc_users_without_tracks():\n",
      "    from db.dbmodel import db_connect, create_tables, ArtistPage, ThirdParty, ArtistSample\n",
      "    from sqlalchemy.orm import sessionmaker\n",
      "    engine = db_connect()\n",
      "    create_tables(engine)\n",
      "    Session = sessionmaker(bind=engine)\n",
      "    s = Session()\n",
      "\n",
      "    left_outer = s.query(ArtistPage).join(ThirdParty).outerjoin(\n",
      "                    ArtistSample, ArtistPage.artist_id == ArtistSample.artist_id\n",
      "                    ).filter(ArtistSample.sample_id == None, ThirdParty.name == 'SoundCloud')\n",
      "\n",
      "    users = {a.page_id: a.artist_id for a in left_outer}\n",
      "\n",
      "    s.close()\n",
      "    return users\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_tracks(self, response):\n",
      "\n",
      "    artist = response.meta['artist']\n",
      "    event = response.meta['event']\n",
      "    tracks = json.loads(response.body)\n",
      "    if tracks:\n",
      "        tracks = [defaultdict(int, t) for t in tracks] #some tracks miss fields\n",
      "        permalinks_plays = [(t['id'], t['playback_count']) for t in tracks]\n",
      "        permalinks_plays.sort(key=lambda x: x[1], reverse=True)\n",
      "\n",
      "        artist['sc_track_id'] = permalinks_plays[0][0]\n",
      "        artist\n",
      "        artist['sc_value'] = sum((p for l, p in permalinks_plays))\n",
      "\n",
      "        event['artists'] = event['artists'] + [artist]\n",
      "        if len(event['artists']) == response.meta['num_artists']: #very hacky take this out!\n",
      "            yield event"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scrapy.contrib.linkextractors import LinkExtractor\n",
      "from scrapy.utils import request\n",
      "from scrapy.http.response import text\n",
      "import scrapy\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scrapy.selector import Selector\n",
      "from scrapy.http import HtmlResponse, Response\n",
      "from requests import get\n",
      "import urllib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://www.residentadvisor.net/event.aspx?638376'\n",
      "url = \"http://www.residentadvisor.net/dj/patrickpoitz\"\n",
      "#url = \"http://api.soundcloud.com/users/agnes/tracks.json?\"+client_id\n",
      "req = get(url)\n",
      "body = get(url).text\n",
      "\n",
      "m = HtmlResponse(url=url, body=body, encoding='utf8')\n",
      "\n",
      "#import json\n",
      "#res = json.loads(m.body)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "response \n",
      "        sc_link_sel = response.xpath(\"//a[contains(@href, 'http://www.soundcloud.com')][contains(text(), 'SoundCloud')]/@href\")\n",
      "        if sc_link_sel:\n",
      "            artist['sc_url'] = sc_link_sel.extract()[0]\n",
      "            artist['sc_user'] = artist['sc_url'].split('/')[-1]\n",
      "        \n",
      "        event['artists'] = event['artists'] + [artist]\n",
      "        if response.meta['num_artists'] == len(event['artists']):\n",
      "            yield event    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g.group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "'632254'"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}