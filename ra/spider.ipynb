{
 "metadata": {
  "name": "",
  "signature": "sha256:21cb36d53417f0847bc245f0787884ee65dfa1741c86401cd6620d31ba78cc42"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%writefile ra/items.py\n",
      "import scrapy\n",
      "\n",
      "class Event(scrapy.Item):\n",
      "    #item_type = scrapy.Field(default='event')\n",
      "    item_type = scrapy.Field()\n",
      "    ra_event_id = scrapy.Field()\n",
      "    ra_url = scrapy.Field()\n",
      "    name = scrapy.Field()    \n",
      "    date = scrapy.Field()\n",
      "    #ra_club_id = scrapy.Field()\n",
      "    \n",
      "    #these fields are for a document-like serving\n",
      "    club = scrapy.Field()\n",
      "    artists = scrapy.Field()\n",
      "    \n",
      "class Club(scrapy.Item):\n",
      "    item_type = scrapy.Field(default='club')\n",
      "    ra_club_id = scrapy.Field()\n",
      "    ra_url = scrapy.Field()\n",
      "    name = scrapy.Field()\n",
      "    adress = scrapy.Field()\n",
      "    latlon = scrapy.Field()\n",
      "    ra_locale_id = scrapy.Field()\n",
      "\n",
      "class Performance(scrapy.Item):\n",
      "    item_type = scrapy.Field(default='performance')\n",
      "    ra_event_id = scrapy.Field()\n",
      "    sc_artist_id = scrapy.Field()\n",
      "    artist = scrapy.Field()\n",
      "    \n",
      "class Artist(scrapy.Item):\n",
      "    item_type = scrapy.Field(default='artist')\n",
      "    ra_artist_id = scrapy.Field()\n",
      "    ra_url = scrapy.Field()\n",
      "    name = scrapy.Field()\n",
      "    sc_user = scrapy.Field()\n",
      "    sc_url = scrapy.Field()\n",
      "    #sc_track_permalink = scrapy.Field()\n",
      "    #sc_track_id = scrapy.Field()\n",
      "    #sc_value = scrapy.Field() #arbitray number for rankin artists\n",
      "    \n",
      "class Track(scrapy.Item):\n",
      "    artist_id = scrapy.Field()\n",
      "    sc_track_id = scrapy.Field()\n",
      "    sc_user_name = scrapy.Field()\n",
      "    sc_num_plays = scrapy.Field()\n",
      "    sc_description = scrapy.Field()\n",
      "    sc_genre = scrapy.Field()\n",
      "    sc_embeddable_by = scrapy.Field()\n",
      "    sc_lable_name = scrapy.Field()\n",
      "    sc_last_modiefied = scrapy.Field()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile ra/spiders/event_spider.py\n",
      "import scrapy\n",
      "from scrapy.contrib.spiders import CrawlSpider, Rule\n",
      "from scrapy.contrib.linkextractors import LinkExtractor\n",
      "scrapy.contrib.linkextractors.lxmlhtml.LxmlLinkExtractor\n",
      "from ra.ra.items import Event, Club, Performance, Artist\n",
      "import urllib\n",
      "import datetime\n",
      "import re\n",
      "import json\n",
      "import geocoder\n",
      "from collections import defaultdict\n",
      "BASE_URL = 'http://www.residentadvisor.net'\n",
      "LISTINGS_EXT = '/events.aspx?'\n",
      "BERLIN_AI = 34\n",
      "TODAY = datetime.date.today()\n",
      "#TODAY = datetime.date(2014, 9, 29)\n",
      "\n",
      "listing_params = {'ai': BERLIN_AI,\n",
      "                  'v': 'day',\n",
      "                  'mn': TODAY.month,\n",
      "                  'yr': TODAY.year,\n",
      "                  'dy': TODAY.day}\n",
      "today_events_url = BASE_URL + LISTINGS_EXT + urllib.urlencode(listing_params)\n",
      "\n",
      "class RAEventSpider(CrawlSpider):\n",
      "    name = 'event_spider'\n",
      "    allowed_domains = [BASE_URL, 'www.residentadvisor.net', 'api.soundcloud.com']\n",
      "    start_urls = [today_events_url]\n",
      "    \n",
      "    rules = [\n",
      "        Rule(LinkExtractor(allow=(r'\\/event\\.aspx\\?',), canonicalize=False),\n",
      "             callback='parse_event'),\n",
      "    ]\n",
      "\n",
      "    extract_digits = re.compile(r'(\\d+)')\n",
      "     \n",
      "    def parse_event(self, response):\n",
      "        '''\n",
      "            @url http://www.residentadvisor.net/events.aspx?ai=34&v=day&mn=10&yr=2014&dy=20\n",
      "            @returns requests 5\n",
      "        '''\n",
      "        \n",
      "        event = Event()\n",
      "        event['ra_url'] = response.url    \n",
      "        event['ra_event_id'] = self.extract_digits.search(event['ra_url']).group(1)\n",
      "        event['item_type'] = 'event'\n",
      "        event_title = response.xpath(\"//div[@id = 'sectionHead']/h1/text()\").extract()[0]\n",
      "        event['name'] = re.match('(.+)\\sat.+$', event_title).group(1) #remove 'at LOCATION'\n",
      "        event['date'] = TODAY.isoformat()\n",
      "        event['artists'] = []\n",
      "       \n",
      "        club_link = response.xpath(\"//a[contains(@title, 'Club profile')]\")\n",
      "        if club_link:\n",
      "            club = Club()\n",
      "            club['ra_url'] = BASE_URL + '/' + club_link.xpath(\"@href\").extract()[0]\n",
      "            club['name'] = club_link.xpath(\"text()\").extract()[0]\n",
      "            id_match = self.extract_digits.search(club['ra_url'])\n",
      "            #self.log(type(id_match))\n",
      "            club['ra_club_id'] = id_match.group(1)\n",
      "            club['adress'] = club_link.xpath(\"../text()\").extract()[0]\n",
      "            \n",
      "            geocode = geocoder.google(club['adress'])\n",
      "            if geocode.status_description == 'OK':\n",
      "                club['latlon'] = geocode.latlng\n",
      "            #event['ra_club_id'] = club['ra_club_id']\n",
      "            event['club'] = club\n",
      "        \n",
      "        lineup_selector = response.css(\".lineup\").xpath(\"a\")\n",
      "        if lineup_selector:\n",
      "            num_artists = len(lineup_selector)\n",
      "            for link_sel in lineup_selector:\n",
      "                artist = Artist()\n",
      "                url_ext = link_sel.xpath(\"@href\").extract()[0]\n",
      "                if url_ext[:4] == \"/dj/\":\n",
      "                    artist['ra_url'] = BASE_URL + url_ext\n",
      "                    artist['name'] = link_sel.xpath(\"text()\").extract()[0]\n",
      "                    artist['ra_artist_id'] = url_ext.split(\"/\")[2]\n",
      "                    \n",
      "                    #step into RA artist page\n",
      "                    request = scrapy.Request(artist['ra_url'], callback=self.parse_artist_page)\n",
      "                    request.meta['event'] = event\n",
      "                    request.meta['artist'] = artist\n",
      "                    request.meta['num_artists'] = num_artists\n",
      "                    yield request\n",
      "        else:\n",
      "            yield event\n",
      "    \n",
      "    def parse_artist_page(self, response):\n",
      "        \n",
      "        artist = response.meta['artist']\n",
      "        event = response.meta['event']\n",
      "        \n",
      "        sc_link_sel = response.xpath(\"//a[contains(@href, 'http://www.soundcloud.com')][contains(text(), 'SoundCloud')]/@href\")\n",
      "        if sc_link_sel:\n",
      "            artist['sc_url'] = sc_link_sel.extract()[0]\n",
      "            artist['sc_user'] = artist['sc_url'].split('/')[-1]\n",
      "        \n",
      "        event['artists'] = event['artists'] + [artist]\n",
      "        if response.meta['num_artists'] == len(event['artists']):\n",
      "            yield event    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting ra/spiders/event_spider.py\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "base = [\"aaa %s bbb\", \"aaa %s bbb\", \"aaa %s bbb\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[(b % \"alec\") for b in base]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "['aaa alec bbb', 'aaa alec bbb', 'aaa alec bbb']"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile ra/spiders/sc_tracks.py\n",
      "import urllib\n",
      "import json\n",
      "from ra.items import Track\n",
      "import scrapy\n",
      "from scrapy.contrib.spiders import CrawlSpider, Rule\n",
      "# artist_id = scrapy.Field()\n",
      "# sc_track_id = scrapy.Field()\n",
      "# sc_user_name = scrapy.Field()\n",
      "# sc_num_plays = scrapy.Field()\n",
      "# sc_description = scrapy.Field()\n",
      "# sc_genre = scrapy.Field()\n",
      "# sc_embeddable_by = scrapy.Field()\n",
      "# sc_lable_name = scrapy.Field()\n",
      "# sc_last_modiefied = scrapy.Field()\n",
      "class SoundCloudSpider(CrawlSpider):\n",
      "    name = 'sc_track_spider'\n",
      "    allowed_domains = ['api.soundcloud.com']\n",
      "\n",
      "    def __init__(self):\n",
      "        with open('sc_cl_id.txt', 'r') as f:\n",
      "            SC_CLIENT_ID = f.read().rstrip()\n",
      "            sc_client_id = urllib.urlencode({\"client_id\": SC_CLIENT_ID})\n",
      "        TRACKS_URL = \"http://api.soundcloud.com/users/%s/tracks.json?\"+sc_client_id\n",
      "        \n",
      "        self.users_dict = get_sc_users_without_tracks()\n",
      "        self.user_names = users_dict.keys()\n",
      "        \n",
      "        self.start_urls = [(TRACKS_URL % user) for user in user_names]\n",
      "\n",
      "    def parse(self, response):\n",
      "        tracks = json.loads(response.body)\n",
      "        if tracks:\n",
      "            for track_dict in tracks:\n",
      "                track_dict =  defaultdict(int, track_dict) # some track miss fields\n",
      "            track = Track()\n",
      "            track['sc_user_name'] = track_dict['user']['username']\n",
      "            track['artist_id'] = users_dict[track['sc_user_name']] #we look up the internal artist id\n",
      "            track['sc_track_id'] = track_dict['id']\n",
      "            track['sc_num_plays'] = track_dict['playback_count']\n",
      "            track['sc_description'] = track_dict['description']\n",
      "            track['sc_genre'] = track_dict['genre']\n",
      "            track['sc_embeddable_by'] = track_dict['embeddable_by']\n",
      "            track['sc_lable_name'] = track_dict['label_name']\n",
      "            track['sc_last_modiefied'] = track_dict['last_modiefied']\n",
      "            yield track\n",
      "            \n",
      "    def get_sc_users_without_tracks(self):\n",
      "        from db.dbmodel import db_connect, create_tables, ArtistPage, ThirdParty, ArtistSample\n",
      "        from sqlalchemy.orm import sessionmaker\n",
      "        \n",
      "        engine = db_connect()\n",
      "        create_tables(engine)\n",
      "        Session = sessionmaker(bind=engine)\n",
      "        s = Session()\n",
      "\n",
      "        left_outer = s.query(ArtistPage).join(ThirdParty).outerjoin(\n",
      "                        ArtistSample, ArtistPage.artist_id == ArtistSample.artist_id\n",
      "                        ).filter(ArtistSample.sample_id == None, ThirdParty.name == 'SoundCloud')\n",
      "\n",
      "        users = {a.page_id: a.artist_id for a in left_outer}\n",
      "\n",
      "        s.close()\n",
      "        return users\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing ra/spiders/sc_tracks.py\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "a = requests.get('http://www.residentadvisor.net/event.aspx?631670')\n",
      "response = HtmlResponse(a.url, body=a.content, encoding='utf8')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 232
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "BASE_URL = 'http://www.residentadvisor.net'\n",
      "extract_digits = re.compile(r'(\\d+)')\n",
      "event = Event()\n",
      "event['ra_url'] = response.url    \n",
      "event['ra_event_id'] = extract_digits.search(event['ra_url']).group(1)\n",
      "event['item_type'] = 'event'\n",
      "event['name'] = response.xpath(\"//div[@id = 'sectionHead']/h1/text()\").extract()[0]\n",
      "event['date'] = None #TODAY.isoformat()\n",
      "event['artists'] = []\n",
      "\n",
      "club_link = response.xpath(\"//a[contains(@title, 'Club profile')]\")\n",
      "if club_link:\n",
      "    club = Club()\n",
      "    club['ra_url'] = BASE_URL + '/' + club_link.xpath(\"@href\").extract()[0]\n",
      "    club['name'] = club_link.xpath(\"text()\").extract()[0]\n",
      "    id_match = extract_digits.search(club['ra_url'])\n",
      "    #self.log(type(id_match))\n",
      "    club['ra_club_id'] = id_match.group(1)\n",
      "    club['adress'] = club_link.xpath(\"../text()\").extract()[0]\n",
      "\n",
      "    #geocode = geocoder.google(club['adress'])\n",
      "    #if geocode.status_description == 'OK':\n",
      "    club['latlon'] = [0, 0] #geocode.latlng\n",
      "    #event['ra_club_id'] = club['ra_club_id']\n",
      "    event['club'] = club\n",
      "\n",
      "lineup_selector = response.css(\".lineup\").xpath(\"a\")\n",
      "if lineup_selector:\n",
      "    num_artists = len(lineup_selector)\n",
      "    for link_sel in lineup_selector:\n",
      "        artist = Artist()\n",
      "        url_ext = link_sel.xpath(\"@href\").extract()[0]\n",
      "        if url_ext[:4] == \"/dj/\":\n",
      "            artist['ra_url'] = BASE_URL + url_ext\n",
      "            artist['name'] = link_sel.xpath(\"text()\").extract()[0]\n",
      "            artist['ra_artist_id'] = url_ext.split(\"/\")[2]\n",
      "\n",
      "            #step into RA artist page\n",
      "            #request = scrapy.Request(artist['ra_url'], callback=self.parse_artist_page)\n",
      "            #request.meta['event'] = event\n",
      "            #request.meta['artist'] = artist\n",
      "            #request.meta['num_artists'] = num_artists"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def date_from_events_url(url):\n",
      "    from urlparse import parse_qs, urlparse\n",
      "    from datetime import date\n",
      "    date_dict =parse_qs(urlparse(url).query, keep_blank_values=True)\n",
      "    try:\n",
      "        y = int(date_dict['yr'][0])\n",
      "        m = int(date_dict['mn'][0])\n",
      "        d = int(date_dict['dy'][0])\n",
      "    except:\n",
      "        raise\n",
      "    return date(y, m , d)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = response.xpath(\"//div[text()='Date /']/..\")\n",
      "def datetimes_from_date_div(date_div):\n",
      "    links = date_div.xpath('a/@href').extract()\n",
      "    num_date_links = len(links)\n",
      "    start_date = date_from_events_url(links[0])\n",
      "    end_date = None\n",
      "    if num_date_links > 1:\n",
      "        end_date = date_from_events_url(links[-1])\n",
      "    \n",
      "    start_time, end_time = times_from_str(date_div.extract()[0])\n",
      "    \n",
      "    start_datetime, end_datetime = join_times_dates(start_date, end_date, start_time, end_time)\n",
      "    \n",
      "    return start_datetime, end_datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 234
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r.extract()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 224,
       "text": [
        "[u'<li><div>Date /</div>Monday<br><a href=\"events.aspx?ai=34&amp;v=day&amp;mn=10&amp;yr=2014&amp;dy=27\">27 October 2014</a><br>23:59 - 09:00</li>']"
       ]
      }
     ],
     "prompt_number": 224
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datetimes_from_date_div(r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 235,
       "text": [
        "(datetime.datetime(2014, 10, 25, 23, 59),\n",
        " datetime.datetime(2014, 10, 27, 6, 0))"
       ]
      }
     ],
     "prompt_number": 235
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def times_from_str(string):\n",
      "    '''\n",
      "        Looks for '12:00 - 13:00' in a string and returns\n",
      "        both times as python time objects.\n",
      "    '''\n",
      "    def time_from_str(time_str):\n",
      "        return datetime.datetime.strptime(time_str, '%H:%M').time()\n",
      "    \n",
      "    match = re.search('(\\d\\d\\:\\d\\d)\\s\\-\\s(\\d\\d\\:\\d\\d)', string)\n",
      "    start_str, end_str = match.group(1), match.group(2)\n",
      "    \n",
      "    start_time = time_from_str(start_str)\n",
      "    end_time = time_from_str(end_str)\n",
      "    \n",
      "    return start_time, end_time\n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 228
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def join_times_dates(start_date, end_date, start_time, end_time):\n",
      "    ''' Takes one or more dates and two times and turns them\n",
      "        into a start datetime and an end datetime.\n",
      "    '''\n",
      "    \n",
      "    \n",
      "    \n",
      "    from datetime import datetime, timedelta\n",
      "    \n",
      "    def same_day(st, et):\n",
      "        return st <= et\n",
      "    def datetime_from_date_time(date, time):\n",
      "        dt = datetime(date.year, date.month, date.day, time.hour, time.minute)\n",
      "        return dt\n",
      "    \n",
      "    \n",
      "    start_datetime = datetime_from_date_time(start_date, start_time)\n",
      "    \n",
      "    # If we didn't receive a end_date and the end time is\n",
      "    # after the start time, the event ends on the same day.\n",
      "    if not end_date and same_day(start_time, end_time):\n",
      "        end_date = start_date\n",
      "    \n",
      "    # If no end_date and end time is before start time\n",
      "    # the event ends one day after the start date\n",
      "    elif not end_date and not same_day(start_time, end_time):\n",
      "        end_date = start_date + timedelta(1) #add one day\n",
      "    \n",
      "    # Otherwise we use the end_date we got passed\n",
      "    else:\n",
      "        pass\n",
      "    \n",
      "    end_datetime = datetime_from_date_time(end_date, end_time)\n",
      "    \n",
      "    return start_datetime, end_datetime\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 210
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = re.search('(\\d\\d\\:\\d\\d)\\s\\-\\s(\\d\\d\\:\\d\\d)', r.extract()[0]).group(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "response.xpath(\"//div[text()='Date /']/..\").extract()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 221,
       "text": [
        "[u'<li><div>Date /</div>Monday<br><a href=\"events.aspx?ai=34&amp;v=day&amp;mn=10&amp;yr=2014&amp;dy=27\">27 October 2014</a><br>23:59 - 09:00</li>']"
       ]
      }
     ],
     "prompt_number": 221
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "date_links = response.xpath(\"//div[text()='Date /']/../a/@href\")\n",
      "if len(date_links) > 1:\n",
      "    pass\n",
      "    #start_date = "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.match('(.+)\\sat.+$', event['name']).group(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "u'Let\\xb4s Stay Awake with Armando de la Fuente'"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "event['name']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "u'Let\\xb4s Stay Awake with Armando de la Fuente at Promenaden Eck'"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sc_users_without_tracks():\n",
      "    from db.dbmodel import db_connect, create_tables, ArtistPage, ThirdParty, ArtistSample\n",
      "    from sqlalchemy.orm import sessionmaker\n",
      "    engine = db_connect()\n",
      "    create_tables(engine)\n",
      "    Session = sessionmaker(bind=engine)\n",
      "    s = Session()\n",
      "\n",
      "    left_outer = s.query(ArtistPage).join(ThirdParty).outerjoin(\n",
      "                    ArtistSample, ArtistPage.artist_id == ArtistSample.artist_id\n",
      "                    ).filter(ArtistSample.sample_id == None, ThirdParty.name == 'SoundCloud')\n",
      "\n",
      "    users = {a.page_id: a.artist_id for a in left_outer}\n",
      "\n",
      "    s.close()\n",
      "    return users"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_tracks(self, response):\n",
      "\n",
      "    artist = response.meta['artist']\n",
      "    event = response.meta['event']\n",
      "    tracks = json.loads(response.body)\n",
      "    if tracks:\n",
      "        tracks = [defaultdict(int, t) for t in tracks] #some tracks miss fields\n",
      "        permalinks_plays = [(t['id'], t['playback_count']) for t in tracks]\n",
      "        permalinks_plays.sort(key=lambda x: x[1], reverse=True)\n",
      "\n",
      "        artist['sc_track_id'] = permalinks_plays[0][0]\n",
      "        artist\n",
      "        artist['sc_value'] = sum((p for l, p in permalinks_plays))\n",
      "\n",
      "        event['artists'] = event['artists'] + [artist]\n",
      "        if len(event['artists']) == response.meta['num_artists']: #very hacky take this out!\n",
      "            yield event"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scrapy.contrib.linkextractors import LinkExtractor\n",
      "from scrapy.utils import request\n",
      "from scrapy.http.response import text\n",
      "import scrapy\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scrapy.selector import Selector\n",
      "from scrapy.http import HtmlResponse, Response\n",
      "from requests import get\n",
      "import urllib"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#url = 'http://www.residentadvisor.net/event.aspx?638376'\n",
      "#url = \"http://www.residentadvisor.net/dj/patrickpoitz\"\n",
      "url = \"http://api.soundcloud.com/users/agnes/tracks.json?\"+client_id\n",
      "req = get(url)\n",
      "body = get(url).text\n",
      "\n",
      "m = HtmlResponse(url=url, body=body, encoding='utf8')\n",
      "\n",
      "import json\n",
      "res = json.loads(m.body)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!rm ../../cm_naive/events/2014-09-27.json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!scrapy crawl event_spider"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/alec/anaconda/lib/python2.7/site-packages/twisted/internet/_sslverify.py:184: UserWarning: You do not have the service_identity module installed. Please install it from <https://pypi.python.org/pypi/service_identity>. Without the service_identity module and a recent enough pyOpenSSL tosupport it, Twisted can perform only rudimentary TLS client hostnameverification.  Many valid certificate/hostname mappings may be rejected.\r\n",
        "  verifyHostname, VerificationError = _selectVerifyImplementation()\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-10-02 14:00:27+0200 [scrapy] INFO: Scrapy 0.24.4 started (bot: ra)\r\n",
        "2014-10-02 14:00:27+0200 [scrapy] INFO: Optional features available: ssl, http11, boto\r\n",
        "2014-10-02 14:00:27+0200 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'ra.spiders', 'SPIDER_MODULES': ['ra.spiders'], 'BOT_NAME': 'ra'}\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-10-02 14:00:27+0200 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2014-10-02 14:00:28+0200 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\r\n",
        "2014-10-02 14:00:28+0200 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\r\n",
        "/home/alec/anaconda/lib/python2.7/site-packages/scrapy/contrib/pipeline/__init__.py:21: ScrapyDeprecationWarning: ITEM_PIPELINES defined as a list or a set is deprecated, switch to a dict\r\n",
        "  category=ScrapyDeprecationWarning, stacklevel=1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Traceback (most recent call last):\r\n",
        "  File \"/home/alec/anaconda/bin/scrapy\", line 6, in <module>\r\n",
        "    sys.exit(execute())\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/cmdline.py\", line 143, in execute\r\n",
        "    _run_print_help(parser, _run_command, cmd, args, opts)\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/cmdline.py\", line 89, in _run_print_help\r\n",
        "    func(*a, **kw)\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/cmdline.py\", line 150, in _run_command\r\n",
        "    cmd.run(args, opts)\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/commands/crawl.py\", line 60, in run\r\n",
        "    self.crawler_process.start()\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/crawler.py\", line 92, in start\r\n",
        "    if self.start_crawling():\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/crawler.py\", line 124, in start_crawling\r\n",
        "    return self._start_crawler() is not None\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/crawler.py\", line 139, in _start_crawler\r\n",
        "    crawler.configure()\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/crawler.py\", line 47, in configure\r\n",
        "    self.engine = ExecutionEngine(self, self._spider_closed)\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/core/engine.py\", line 65, in __init__\r\n",
        "    self.scraper = Scraper(crawler)\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/core/scraper.py\", line 66, in __init__\r\n",
        "    self.itemproc = itemproc_cls.from_crawler(crawler)\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/middleware.py\", line 50, in from_crawler\r\n",
        "    return cls.from_settings(crawler.settings, crawler)\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/middleware.py\", line 29, in from_settings\r\n",
        "    mwcls = load_object(clspath)\r\n",
        "  File \"/home/alec/anaconda/lib/python2.7/site-packages/scrapy/utils/misc.py\", line 42, in load_object\r\n",
        "    raise ImportError(\"Error loading object '%s': %s\" % (path, e))\r\n",
        "ImportError: Error loading object 'ra.pipelines.RaPipeline': No module named db.dbmodel\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat a.json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{\"name\": \"House of Waxx at Tresor\", \"url\": \"http://www.residentadvisor.net/event.aspx?629290\", \"club\": {\"url\": \"club.aspx?id=5494\", \"latlon\": [52.5108799, 13.4201002], \"ra_club_id\": \"5494\", \"adress\": \" K\\u00f6penickerstrasse 70; Mitte; 10179 Berlin;\", \"name\": \"Tresor\"}, \"artists\": [{\"ra_artist_id\": \"gunnarstiller\", \"sc_track_permalink\": \"http://soundcloud.com/gunnar-stiller/panoramabar_2011-10-23\", \"name\": \"Gunnar Stiller\", \"sc_user\": \"gunnar-stiller\", \"url\": \"http://www.residentadvisor.net/dj/gunnarstiller\", \"sc_value\": 112417, \"sc_link\": \"http://www.soundcloud.com/gunnar-stiller\"}, {\"ra_artist_id\": \"axelbartsch\", \"sc_track_permalink\": \"http://soundcloud.com/axelbartsch/katerholzig-daytime-dj-action\", \"name\": \"Axel Bartsch\", \"sc_user\": \"axelbartsch\", \"url\": \"http://www.residentadvisor.net/dj/axelbartsch\", \"sc_value\": 38713, \"sc_link\": \"http://www.soundcloud.com/axelbartsch\"}], \"date\": \"2014-09-29\", \"ra_club_id\": \"5494\", \"ra_event_id\": \"629290\"},\r\n",
        "{\"url\": \"http://www.residentadvisor.net/event.aspx?629227\", \"date\": \"2014-09-29\", \"artists\": [{\"ra_artist_id\": \"tiny\", \"sc_track_permalink\": \"http://soundcloud.com/tiney/done-by-the-dawgs\", \"name\": \"Tiny\", \"sc_user\": \"tiney\", \"url\": \"http://www.residentadvisor.net/dj/tiny\", \"sc_value\": 3077, \"sc_link\": \"http://www.soundcloud.com/tiney\"}], \"ra_event_id\": \"629227\", \"name\": \"Tiny Ruins at Gr\\u00fcner Salon Der Volksb\\u00fchne\"},\r\n",
        "{\"name\": \"Let's Stay Awake mit Ollider Twist at Promenaden Eck\", \"url\": \"http://www.residentadvisor.net/event.aspx?632989\", \"club\": {\"url\": \"club.aspx?id=88900\", \"latlon\": [52.475866, 13.4220073], \"ra_club_id\": \"88900\", \"adress\": \" Schillerpromenade 11, 12049 Berlin\", \"name\": \"Promenaden Eck\"}, \"artists\": [{\"ra_artist_id\": \"ollidertwist\", \"sc_track_permalink\": \"http://soundcloud.com/ollider-twist/ick-hau-van-jau\", \"name\": \"Ollider Twist\", \"sc_user\": \"ollider-twist\", \"url\": \"http://www.residentadvisor.net/dj/ollidertwist\", \"sc_value\": 1682, \"sc_link\": \"http://www.soundcloud.com/https://ollider-twist\"}], \"date\": \"2014-09-29\", \"ra_club_id\": \"88900\", \"ra_event_id\": \"632989\"},\r\n",
        "{\"name\": \"Electro Funk Roots at Cake Club\", \"url\": \"http://www.residentadvisor.net/event.aspx?628859\", \"club\": {\"url\": \"club.aspx?id=42409\", \"latlon\": [52.5016358, 13.41866], \"ra_club_id\": \"42409\", \"adress\": \" Oranienstra\\u00dfe 32 10999 Berlin\", \"name\": \"Cake Club\"}, \"artists\": [{\"ra_artist_id\": \"djdannyrussell\", \"sc_track_permalink\": \"http://soundcloud.com/dj-danny-russell/electro-funk-roots-mix\", \"name\": \"DJ Danny Russell\", \"sc_user\": \"dj-danny-russell\", \"url\": \"http://www.residentadvisor.net/dj/djdannyrussell\", \"sc_value\": 1622, \"sc_link\": \"http://www.soundcloud.com/dj-danny-russell\"}], \"date\": \"2014-09-29\", \"ra_club_id\": \"42409\", \"ra_event_id\": \"628859\"}]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "id_extract=re.compile(r'(\\d+)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g = id_extract.search(\"http://www.residentadvisor.net/event.aspx?632254\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g.group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "'632254'"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}